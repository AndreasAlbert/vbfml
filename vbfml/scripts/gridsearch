#!/usr/bin/env python3
import copy
import os
import re
import shutil
import itertools
import warnings
import logging

import click
import tensorflow as tf
import numpy as np
import pandas as pd

from keras import backend as K
from datetime import datetime
from tabulate import tabulate
from typing import List
from tqdm import tqdm
from multiprocessing.pool import Pool

from vbfml.models import sequential_dense_model, sequential_convolutional_model
from vbfml.training.data import TrainingLoader
from vbfml.training.input import build_sequence, load_datasets_bucoffea
from vbfml.helpers.deployment import pack_repo
from vbfml.helpers.condor import condor_submit

from vbfml.training.util import (
    append_history,
    normalize_classes,
    save,
    select_and_label_datasets,
    PrintingCallback,
)

from vbfml.util import (
    ModelConfiguration,
    ModelFactory,
    vbfml_path,
    git_rev_parse,
    git_diff,
    git_diff_staged,
)

pjoin = os.path.join

# Ignore pandas performance warnings for now
warnings.filterwarnings("ignore", category=pd.errors.PerformanceWarning)

# Tensorflow can be more silent
# tf.autograph.set_verbosity(0)
logging.getLogger("tensorflow").setLevel(logging.ERROR)


def get_gridsearch_directory(tag: str) -> str:
    return pjoin("./output", f"gridsearch_{tag}")


@click.group()
@click.option(
    "--tag",
    default=datetime.now().strftime("%Y-%m-%d_%H-%M"),
    required=False,
    help="A string-valued tag used to identify the run. If a run with this tag exists, will use existing run.",
)
@click.option(
    "--input-dir",
    default=vbfml_path("root/2021-11-13_vbfhinv_treesForML"),
    required=False,
    help="Input directory containing the ROOT files for training and validation.",
)
@click.option(
    "--model-config",
    default=vbfml_path("config/convolutional_model.yml"),
    required=False,
    help="Path to the .yml file that has the model configuration parameters.",
)
@click.option(
    "--frac-qcdv-events",
    default=0.5,
    required=False,
    help="Fraction of the QCD V events to use.",
)
@click.pass_context
def cli(ctx, tag: str, input_dir: str, model_config: str, frac_qcdv_events: float):
    ctx.ensure_object(dict)
    ctx.obj["TAG"] = tag
    ctx.obj["INPUT_DIR"] = input_dir
    ctx.obj["MODEL_CONFIG"] = model_config
    ctx.obj["FRAC_QCDV_EVENTS"] = frac_qcdv_events


def do_setup(
    output_directory: str, input_dir: str, model_config: str, frac_qcdv_events: float
):
    """
    Creates a new working area with training and valdation sequences.
    Prerequisite for later training.
    """
    all_datasets = load_datasets_bucoffea(directory=input_dir)

    dataset_labels = {
        "ewk_17": "(EWK.*2017|VBF_HToInvisible_M125_withDipoleRecoil_pow_pythia8_2017)",
        "v_qcd_nlo_17": "(WJetsToLNu_Pt-\d+To.*|Z\dJetsToNuNu_M-50_LHEFilterPtZ-\d+To\d+)_MatchEWPDG20-amcatnloFXFX_2017",
    }
    datasets = select_and_label_datasets(all_datasets, dataset_labels)
    # Use X% of QCD V events, as specified from the command line
    # Default is 50% (should be 3-4 hours runtime on a GPU)
    for dataset_info in datasets:
        if re.match(dataset_labels["v_qcd_nlo_17"], dataset_info.name):
            dataset_info.n_events = int(
                np.floor(frac_qcdv_events * dataset_info.n_events)
            )

    # Object containing data for different models
    # (set of features, dropout rate etc.)
    # Loaded from the YML configuration file
    mconfig = ModelConfiguration(model_config)

    features = mconfig.get("features")

    training_params = mconfig.get("training_parameters")
    validation_params = mconfig.get("validation_parameters")

    training_sequence = build_sequence(
        datasets=copy.deepcopy(datasets),
        features=features,
        weight_expression=mconfig.get("weight_expression"),
        shuffle=training_params["shuffle"],
        scale_features=training_params["scale_features"],
    )
    validation_sequence = build_sequence(
        datasets=copy.deepcopy(datasets),
        features=features,
        weight_expression=mconfig.get("weight_expression"),
        shuffle=validation_params["shuffle"],
        scale_features=validation_params["scale_features"],
    )
    normalize_classes(training_sequence)
    normalize_classes(validation_sequence)

    # Training sequence
    train_size = training_params["train_size"]

    training_sequence.read_range = (0.0, train_size)
    training_sequence.batch_size = training_params["batch_size"]
    training_sequence.batch_buffer_size = training_params["batch_buffer_size"]
    training_sequence[0]

    # Validation sequence
    validation_sequence.read_range = (train_size, 1.0)
    validation_sequence.scale_features = True
    validation_sequence._feature_scaler = copy.deepcopy(
        training_sequence._feature_scaler
    )
    validation_sequence.batch_size = validation_params["batch_size"]
    validation_sequence.batch_buffer_size = validation_params["batch_buffer_size"]

    try:
        os.makedirs(output_directory)
    except FileExistsError:
        pass

    def prepend_path(fname):
        return pjoin(output_directory, fname)

    # Feature scaling object for future evaluation
    save(training_sequence._feature_scaler, prepend_path("feature_scaler.pkl"))

    # List of features
    save(
        features,
        prepend_path(
            "features.pkl",
        ),
    )

    # Training and validation sequences
    # Clear buffer before saving to save space
    for seq in training_sequence, validation_sequence:
        seq.buffer.clear()
    save(training_sequence, prepend_path("training_sequence.pkl"))
    save(validation_sequence, prepend_path("validation_sequence.pkl"))

    # Save model type identifier for later uses
    with open(prepend_path("model_identifier.txt"), "w+") as f:
        f.write(mconfig.get("architecture"))


def create_model(learning_rate: float, n_features: int, n_classes: int, dropout: float):
    model = sequential_dense_model(
        n_layers=3,
        n_nodes=[4, 4, 2],
        n_features=n_features,
        n_classes=n_classes,
        dropout=dropout,
    )
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=learning_rate,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07,
        amsgrad=False,
        name="Adam",
    )

    cce = tf.keras.losses.CategoricalCrossentropy(name="cce")

    model.compile(
        loss="categorical_crossentropy",
        optimizer=optimizer,
        weighted_metrics=["categorical_accuracy"],
    )

    return model


def create_convolutional_model(
    n_layers_for_conv: int,
    num_filters: int,
    kernel_size: int,
    n_layers_for_dense: int,
    num_nodes_for_dense: int,
    dropout: float,
):
    model = sequential_convolutional_model(
        n_layers_for_conv=n_layers_for_conv,
        n_filters_for_conv=[num_filters] * n_layers_for_conv,
        filter_size_for_conv=[kernel_size] * n_layers_for_conv,
        pool_size_for_conv=[2],
        n_layers_for_dense=n_layers_for_dense,
        n_nodes_for_dense=[num_nodes_for_dense] * n_layers_for_dense,
        dropout=dropout,
        image_shape=(40, 20, 1),
        n_classes=2,
    )

    optimizer = tf.keras.optimizers.Adam(
        # learning_rate=learning_rate,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07,
        amsgrad=False,
        name="Adam",
    )

    model.compile(
        loss="categorical_crossentropy",
        optimizer=optimizer,
        weighted_metrics=["categorical_accuracy"],
    )

    return model


@cli.command()
@click.pass_context
@click.option(
    "--steps-per-epoch",
    type=int,
    default=int(1e4),
    help="Number of batches in an epoch.",
)
@click.option(
    "--training-passes",
    type=int,
    default=10,
    help="Number of iterations through the whole training set.",
)
@click.option(
    "--dryrun",
    is_flag=True,
    help="Dry run flag, will create the files but won't submit anything.",
)
@click.option(
    "--test", is_flag=True, help="Test run flag, will submit only up to 5 jobs."
)
@click.option(
    "--overwrite",
    is_flag=True,
    help="""
    If a model directory already exists, overwrite it. 
    If this is not specified, that will cause a RuntimeError instead.
    """,
)
def search(
    ctx,
    steps_per_epoch: int,
    training_passes: int,
    dryrun: bool,
    test: bool,
    overwrite: bool,
):
    """
    Run grid search in a previously created working area.

    Will prepare models with the given parameter set, and will send
    training jobs to HTcondor to train each model.
    """
    import htcondor

    NUM_TEST_JOBS = 5

    output_directory = get_gridsearch_directory(ctx.obj["TAG"])

    # Set up the working area in a temporary directory
    # We will set up the sequences once and for each model directory,
    # simply copy the pkl files over.
    temp_dir = pjoin(output_directory, "temp")
    do_setup(
        output_directory=temp_dir,
        input_dir=ctx.obj["INPUT_DIR"],
        model_config=ctx.obj["MODEL_CONFIG"],
        frac_qcdv_events=ctx.obj["FRAC_QCDV_EVENTS"],
    )

    param_grid = {
        "n_layers_for_conv": [2],
        "num_filters": [32, 64],
        "kernel_size": [3],
        "n_layers_for_dense": [2, 3],
        "num_nodes_for_dense": [200, 300],
        "dropout": [0.25, 0.5],
    }

    # Get all possible parameter combinations
    keys, values = zip(*param_grid.items())
    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]

    models = {}
    jdl_to_submit = []

    num_jobs = NUM_TEST_JOBS if test else len(combinations)

    for i in tqdm(range(num_jobs), desc="Setting up workspaces"):
        key = f"model_v{i}"

        model_directory = pjoin(output_directory, key)
        if os.path.exists(model_directory) and not overwrite:
            raise RuntimeError(f"Model directory already exists: {model_directory}")

        try:
            os.makedirs(model_directory)
        except FileExistsError:
            pass

        # Copy over the sequence files to this directory
        sequence_files = [
            pjoin(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith(".pkl")
        ]
        for f in sequence_files:
            shutil.copy(f, model_directory)

        models[key] = create_convolutional_model(**combinations[i])

        # Tabulate the model parameters and dump to a file
        parameters = combinations[i]
        table = []
        for k, v in parameters.items():
            table.append([k, v])

        model_info_file = pjoin(model_directory, "version.txt")
        with open(model_info_file, "w+") as f:
            f.write(f"Model v{i} parameters:\n")
            f.write(tabulate(table, headers=["Parameter Name", "Value"]))
            f.write("\n")

        # Save the model to the relevant directory
        models[key].save(
            pjoin(model_directory, "models/latest"), include_optimizer=True
        )

        # List of input files/directories we want to ship to the execution machine
        input_files = []

        # Pack the repository for deployment to execution machine
        gridpack_path = pjoin(model_directory, "vbfml.tgz")
        if not os.path.exists(gridpack_path):
            pack_repo(gridpack_path, os.path.abspath(model_directory))
        input_files.append(os.path.abspath(gridpack_path))

        # Submission details
        filedir = pjoin(model_directory, "files")
        if not os.path.exists(filedir):
            os.makedirs(filedir)

        # Output path to transfer to local machine:
        # The directory containing the trained model
        training_dir_name = os.path.basename(model_directory.rstrip("/"))
        output_paths = [training_dir_name]

        # Arguments to the shell script
        arguments = [
            f"--training-directory {os.path.relpath(model_directory)}",
            "train",
            f"--steps-per-epoch {steps_per_epoch}",
            f"--training-passes {training_passes}",
            "--no-verbose-output",
        ]

        submission_settings = {
            "Initialdir": os.path.abspath(output_directory),
            "executable": vbfml_path("execute/htcondor_wrap.sh"),
            "arguments": " ".join(arguments),
            "should_transfer_files": "YES",
            "when_to_transfer_output": "ON_EXIT",
            "transfer_input_files": ", ".join(input_files),
            "transfer_output_files": ", ".join(output_paths),
            "Output": os.path.abspath(pjoin(filedir, "out.txt")),
            "Error": os.path.abspath(pjoin(filedir, "err.txt")),
            "log": os.path.abspath(pjoin(filedir, "log.txt")),
            "request_cpus": 1,
            "request_memory": 2500,
            "+MaxRuntime": f"{60*60*24}",
            # "on_exit_remove" : "((ExitBySignal == False) && (ExitCode == 0)) || (NumJobStarts >= 2)",
            # "request_GPUs": 1,
        }

        # Prepare the jdl job file
        sub = htcondor.Submit(submission_settings)
        jdl = pjoin(filedir, "job_file.jdl")
        with open(jdl, "w") as f:
            f.write(str(sub))
            f.write("\nqueue 1\n")

        jdl_to_submit.append(jdl)

    # Cleanup the temp directory
    shutil.rmtree(temp_dir)

    # Job submission
    if dryrun:
        print("Dry run completed.")
    else:
        print(f"Starting asynchronous submission.")
        p = Pool(processes=8)
        res = p.map_async(condor_submit, jdl_to_submit)
        res.wait()
        if res.successful():
            print(f"Asynchronous submission successful for {len(jdl_to_submit)} jobs.")
        else:
            print("Asynchronous submission failed.")


if __name__ == "__main__":
    cli()
