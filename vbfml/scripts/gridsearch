#!/usr/bin/env python3
import copy
import os
import re
from datetime import datetime

import click
import tensorflow as tf
import numpy as np
from keras import backend as K
from vbfml.models import sequential_dense_model, sequential_convolutional_model
from vbfml.training.data import TrainingLoader
from vbfml.training.input import build_sequence, load_datasets_bucoffea
from vbfml.training.util import (
    append_history,
    normalize_classes,
    save,
    select_and_label_datasets,
)

from vbfml.util import (
    ModelConfiguration,
    ModelFactory,
    vbfml_path,
    git_rev_parse,
    git_diff,
    git_diff_staged,
)

def get_gridsearch_directory(tag: str) -> str:
    return os.path.join("./output", f"gridsearch_{tag}")


@click.group()
@click.option(
    "--tag",
    default=datetime.now().strftime("%Y-%m-%d_%H-%M"),
    required=False,
    help="A string-valued tag used to identify the run. If a run with this tag exists, will use existing run.",
)
@click.pass_context
def cli(ctx, tag):
    ctx.ensure_object(dict)
    ctx.obj["TAG"] = tag


@cli.command()
@click.pass_context
@click.option(
    "--input-dir",
    default=vbfml_path("root/2021-11-13_vbfhinv_treesForML"),
    required=False,
    help="Input directory containing the ROOT files for training and validation.",
)
@click.option(
    "--model-config",
    default=vbfml_path("config/convolutional_model.yml"),
    required=False,
    help="Path to the .yml file that has the model configuration parameters.",
)
def setup(ctx, input_dir: str, model_config: str):
    """
    Creates a new working area. Prerequisite for later training.
    """
    all_datasets = load_datasets_bucoffea(directory=input_dir)

    dataset_labels = {
        "ewk_17": "(EWK.*2017|VBF_HToInvisible_M125_withDipoleRecoil_pow_pythia8_2017)",
        "v_qcd_nlo_17": "(WJetsToLNu_Pt-\d+To.*|Z\dJetsToNuNu_M-50_LHEFilterPtZ-\d+To\d+)_MatchEWPDG20-amcatnloFXFX_2017",
    }
    datasets = select_and_label_datasets(all_datasets, dataset_labels)
    for dataset_info in datasets:
        if re.match(dataset_labels["v_qcd_nlo_17"], dataset_info.name):
            dataset_info.n_events = int(np.floor(0.01 * dataset_info.n_events))

    # Object containing data for different models
    # (set of features, dropout rate etc.)
    # Loaded from the YML configuration file
    mconfig = ModelConfiguration(model_config)

    features = mconfig.get("features")

    training_params = mconfig.get("training_parameters")
    validation_params = mconfig.get("validation_parameters")

    training_sequence = build_sequence(
        datasets=copy.deepcopy(datasets),
        features=features,
        weight_expression=mconfig.get("weight_expression"),
        shuffle=training_params["shuffle"],
        scale_features=training_params["scale_features"],
    )
    validation_sequence = build_sequence(
        datasets=copy.deepcopy(datasets),
        features=features,
        weight_expression=mconfig.get("weight_expression"),
        shuffle=validation_params["shuffle"],
        scale_features=validation_params["scale_features"],
    )
    normalize_classes(training_sequence)
    normalize_classes(validation_sequence)

    # Training sequence
    train_size = training_params["train_size"]
    
    training_sequence.read_range = (0.0, train_size)
    training_sequence.batch_size = training_params["batch_size"]
    training_sequence.batch_buffer_size = training_params["batch_buffer_size"]
    training_sequence[0]

    # Validation sequence
    validation_sequence.read_range = (train_size, 1.0)
    validation_sequence.scale_features = True
    validation_sequence._feature_scaler = copy.deepcopy(
        training_sequence._feature_scaler
    )
    validation_sequence.batch_size = validation_params["batch_size"]
    validation_sequence.batch_buffer_size = validation_params["batch_buffer_size"]

    output_directory = get_gridsearch_directory(ctx.obj["TAG"])

    try:
        os.makedirs(output_directory)
    except FileExistsError:
        pass

    def prepend_path(fname):
        return os.path.join(output_directory, fname)

    # Feature scaling object for future evaluation
    save(training_sequence._feature_scaler, prepend_path("feature_scaler.pkl"))

    # List of features
    save(
        features,
        prepend_path(
            "features.pkl",
        ),
    )
    with open("features.txt", "w") as f:
        f.write("\n".join(features) + "\n")

    # Training and validation sequences
    # Clear buffer before saving to save space
    for seq in training_sequence, validation_sequence:
        seq.buffer.clear()
    save(training_sequence, prepend_path("training_sequence.pkl"))
    save(validation_sequence, prepend_path("validation_sequence.pkl"))


def create_model(learning_rate: float, n_features: int, n_classes: int, dropout: float):
    model = sequential_dense_model(
        n_layers=3,
        n_nodes=[4, 4, 2],
        n_features=n_features,
        n_classes=n_classes,
        dropout=dropout,
    )
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=learning_rate,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07,
        amsgrad=False,
        name="Adam",
    )

    cce = tf.keras.losses.CategoricalCrossentropy(name="cce")

    model.compile(
        loss="categorical_crossentropy",
        optimizer=optimizer,
        weighted_metrics=["categorical_accuracy", "accuracy", cce.__call__],
    )

    return model

def create_convolutional_model(dropout: float, l2_reg_factor: float):
    model = sequential_convolutional_model(
        n_layers_for_conv=1,
        n_filters_for_conv=[4],
        filter_size_for_conv=[4],
        pool_size_for_conv=[2],
        n_layers_for_dense=1,
        n_nodes_for_dense=[100],
        image_shape=(40,20,1),
        n_classes=2,
        dropout=dropout,
        l2_reg_factor=l2_reg_factor,
    )

    optimizer = tf.keras.optimizers.Adam(
        # learning_rate=learning_rate,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07,
        amsgrad=False,
        name="Adam",
    )

    cce = tf.keras.losses.CategoricalCrossentropy(name="cce")

    model.compile(
        loss="categorical_crossentropy",
        optimizer=optimizer,
        weighted_metrics=["categorical_accuracy", "accuracy", cce.__call__],
    )

    return model


@cli.command()
@click.pass_context
def search(ctx):
    """
    Run grid search in a previously created working area.
    """
    output_directory = get_gridsearch_directory(ctx.obj["TAG"])

    # Retrieve the training+validation sequences from the training directory,
    # which is setup earlier
    loader = TrainingLoader(output_directory)

    training_sequence = loader.get_sequence("training")
    validation_sequence = loader.get_sequence("validation")
    assert training_sequence._feature_scaler
    assert validation_sequence._feature_scaler
    steps_total = len(training_sequence)

    features = loader.get_features()

    models = {}
    for l2_reg_factor in [1e-8, 1e-7, 1e-6]:
        for dropout in [0, 0.2, 0.4]:
            key = f"l2-{str(l2_reg_factor)[-1]}_do-{dropout}"
            models[key] = create_convolutional_model(
                dropout=dropout,
                l2_reg_factor=l2_reg_factor,
            )

    steps_total = len(training_sequence)
    steps_per_epoch = 1000
    training_passes = 10
    epochs = training_passes * steps_total // steps_per_epoch
    validation_freq = epochs // (training_passes)

    model_directory = os.path.join(output_directory, "models")
    try:
        os.makedirs(model_directory)
    except FileExistsError:
        pass
    for key, model in models.items():
        history = model.fit(
            x=training_sequence,
            steps_per_epoch=steps_per_epoch,
            epochs=epochs,
            max_queue_size=0,
            shuffle=False,
            validation_data=validation_sequence,
            validation_freq=validation_freq,
        )
        model.save(os.path.join(model_directory, key))

        history = append_history(
            {}, model.history.history, validation_frequence=validation_freq
        )
        save(history, os.path.join(output_directory, f"history_{key}.pkl"))


if __name__ == "__main__":
    cli()
